{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xVbvaj_phyN"
      },
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "outputs": [],
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      },
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      },
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1QCaw6cqDnn"
      },
      "outputs": [],
      "source": [
        "# Ваш код здесь\n",
        "import re\n",
        "\n",
        "def tokenization(text):\n",
        "    tokens = re.split(r'[^\\w]+', text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThvPcovqgO5"
      },
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14BIv33iqrkL"
      },
      "outputs": [],
      "source": [
        "# Ваш код здесь\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      },
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0NQg-VfuFW_"
      },
      "outputs": [],
      "source": [
        "# Ваш код здесь\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tokenize_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmyJfB9wuKkm"
      },
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvUmk94MhrL8"
      },
      "outputs": [],
      "source": [
        "# Ваш код здесь\n",
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\") # Загрузка необходимых библиотек\n",
        "\n",
        "def tokenization(text): # Функция для токенизации по пробелам и знакам препинания\n",
        "    tokens = re.split(r'[^\\w]+', text)\n",
        "    return tokens\n",
        "\n",
        "def tokenize_nltk(text): # Функция для токенизации через nltk\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def tokenize_spacy(text): # Функция для токенизации через spacy\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "for i in text: # цикл, в котором для каждого текста из списка text, будет применен каждый метод\n",
        "    print(tokenization(i))\n",
        "    print(tokenize_nltk(i))\n",
        "    print(tokenize_spacy(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwe1Co6MvibX"
      },
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgE2bQFXv0MG"
      },
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "Ответ: Такое разделение не сможет в полной мере передать структуру рассматриваемого языка. Поэтому подобная система будет иметь определенные последствия при использовании, например, с китайским или японским языками, которые используют иероглифы и имеет иную структуру, в отличие от английского или русского. Если же взять английский язык, то могут возникнуть трудности с сокращениями или дефисами, например в словах: don't, Mark's, e-mail. Также этот подход не подойдет для работы с ссылками на сайты.\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "Ответ: GPT-4 и GPT-5 используют одинаковый токенайзер (tiktoken). Таким образом, в данной фразе будет 10 токенов, 1 токен на каждое английское слово, знаков препинания во фразе нет. Источники: https://platform.openai.com/tokenizer, https://token-counter.app/openai/gpt-5\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)\n",
        "\n",
        "Ответ: Данный алгоритм разбивает слова на субслова, раскладывая каждое слово на буквы и находя самые частые сочетания букв. Алгоритм находит корень слова и остальные морфемы. Например, слова low и lowest. Алгоритм найдет соответсвие и разделит слово lowest на два токена: low и est. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
