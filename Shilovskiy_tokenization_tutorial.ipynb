{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7f7aac47",
      "metadata": {
        "id": "7f7aac47"
      },
      "source": [
        "\n",
        "# Токенизация в NLP\n",
        "\n",
        "## Введение\n",
        "\n",
        "**Токенизация** – это процесс разбиения текста на минимальные единицы (токены): слова, подслова или символы.  \n",
        "Она является первым шагом во многих NLP задачах: анализ тональности, машинный перевод, чат-боты, обучение языковых моделей.\n",
        "\n",
        "В этом туториале мы разберём разные виды токенизации и библиотеки, которые их реализуют.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff830a7",
      "metadata": {
        "id": "3ff830a7"
      },
      "source": [
        "\n",
        "## 1. Простые методы токенизации\n",
        "\n",
        "### 1.1. Разделение по пробелам\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c497c38f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c497c38f",
        "outputId": "bf1761bd-3088-4a0d-f3a3-fc06da16bb47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello,', 'world!', 'This', 'is', 'a', 'test.']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "text = \"Hello, world! This is a test.\"\n",
        "text.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad40dfc0",
      "metadata": {
        "id": "ad40dfc0"
      },
      "source": [
        "\n",
        "### 1.2. Регулярные выражения\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ad4956",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11ad4956",
        "outputId": "dd518f6f-32c5-4136-947a-152d77e04872"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'world', 'This', 'is', 'a', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world! This is a test.\"\n",
        "tokens = re.findall(r\"\\w+\", text)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c829e88",
      "metadata": {
        "id": "7c829e88"
      },
      "source": [
        "\n",
        "**Ограничения простых методов**:  \n",
        "- Не учитывают пунктуацию.  \n",
        "- Не работают с разными языками и морфологией.  \n",
        "- Не подходят для серьёзных задач.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0bbff7",
      "metadata": {
        "id": "8e0bbff7"
      },
      "source": [
        "\n",
        "## 2. Токенизация с помощью библиотек\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50303eea",
      "metadata": {
        "id": "50303eea"
      },
      "source": [
        "\n",
        "### 2.1. NLTK  \n",
        "**Применение**: образовательные проекты, простая предобработка текста.  \n",
        "**Ограничения**: работает медленно на больших корпусах.  \n",
        "**Когда использовать**: для курсовых и лабораторных работ по основам NLP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ed988a",
      "metadata": {
        "id": "a4ed988a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello, world! This is a test.\"\n",
        "print(word_tokenize(text))\n",
        "print(sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8854c741",
      "metadata": {
        "id": "8854c741"
      },
      "source": [
        "\n",
        "### 2.2. spaCy  \n",
        "**Применение**: промышленные проекты, морфология, синтаксис.  \n",
        "**Ограничения**: большие модели, требует ресурсов.  \n",
        "**Когда использовать**: для курсовых по синтаксису и морфологии, проектов по обработке естественного языка.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13d3373c",
      "metadata": {
        "id": "13d3373c"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Hello, world! This is a test.\")\n",
        "[t.text for t in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9f9ee4",
      "metadata": {
        "id": "0b9f9ee4"
      },
      "source": [
        "\n",
        "### 2.3. Stanza  \n",
        "**Применение**: многоязычные корпуса, морфология, синтаксис.  \n",
        "**Ограничения**: медленнее spaCy, требует загрузки моделей.  \n",
        "**Когда использовать**: в исследованиях и при работе с редкими языками.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8367d55c",
      "metadata": {
        "id": "8367d55c"
      },
      "outputs": [],
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "\n",
        "stanza.download(\"en\")\n",
        "nlp = stanza.Pipeline(\"en\")\n",
        "doc = nlp(\"Hello, world! This is a test.\")\n",
        "[[word.text for word in sent.words] for sent in doc.sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e30a0fb",
      "metadata": {
        "id": "1e30a0fb"
      },
      "source": [
        "\n",
        "### 2.4. MosesTokenizer (sacremoses)  \n",
        "**Применение**: машинный перевод, предобработка текстов.  \n",
        "**Ограничения**: устаревающий стандарт.  \n",
        "**Когда использовать**: для экспериментов с классическими MT-системами.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8809846c",
      "metadata": {
        "id": "8809846c"
      },
      "outputs": [],
      "source": [
        "!pip install sacremoses\n",
        "\n",
        "from sacremoses import MosesTokenizer\n",
        "\n",
        "mt = MosesTokenizer()\n",
        "mt.tokenize(\"Hello, world! This is a test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0670bd0",
      "metadata": {
        "id": "c0670bd0"
      },
      "source": [
        "\n",
        "### 2.5. gensim  \n",
        "**Применение**: подготовка данных для моделей `word2vec`, `fastText`.  \n",
        "**Ограничения**: базовая токенизация.  \n",
        "**Когда использовать**: при обучении собственных векторных моделей.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d521d38d",
      "metadata": {
        "id": "d521d38d"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "simple_preprocess(\"Hello, world! This is a test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a097e4e2",
      "metadata": {
        "id": "a097e4e2"
      },
      "source": [
        "\n",
        "### 2.6. HuggingFace Tokenizers  \n",
        "**Применение**: трансформеры, нейросетевые модели.  \n",
        "**Ограничения**: сложность, нужен GPU для обучения токенизаторов.  \n",
        "**Когда использовать**: в курсовых и проектах с BERT, GPT, T5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8788801d",
      "metadata": {
        "id": "8788801d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer(\"Hello, world! This is a test.\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7ee318",
      "metadata": {
        "id": "4b7ee318"
      },
      "source": [
        "\n",
        "## 3. Посимвольная токенизация  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24e768a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d24e768a",
        "outputId": "5176c8ee-0755-4985-bd27-b41eba2904a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "list(\"Hello, world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb08a046",
      "metadata": {
        "id": "bb08a046"
      },
      "source": [
        "\n",
        "## 4. Комбинированные методы  \n",
        "\n",
        "- **Послоговая токенизация** (актуально для языков с чёткой слоговой структурой, например, японский).  \n",
        "- **Гибридные методы**: совмещение слов и символов.  \n",
        "\n",
        "Пример: статья *Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models*.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311105c2",
      "metadata": {
        "id": "311105c2"
      },
      "source": [
        "\n",
        "## 5. Сравнение токенизаторов  \n",
        "\n",
        "Попробуем применить разные методы к одному и тому же тексту.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816fdf03",
      "metadata": {
        "id": "816fdf03"
      },
      "source": [
        "\n",
        "## 6. Практика\n",
        "\n",
        "Возьмём новостной текст, токенизируем разными способами и сравним.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60a714e",
      "metadata": {
        "id": "d60a714e"
      },
      "outputs": [],
      "source": [
        "article = 'Natural language processing (NLP) is a field of artificial intelligence \\\n",
        "that gives computers the ability to understand text and spoken words in much the same way human beings can.'\n",
        "\n",
        "\"\"\"\n",
        "NLTK\n",
        "\"\"\"\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "print(word_tokenize(article))\n",
        "print(sent_tokenize(article))\n",
        "\n",
        "\"\"\"\n",
        "SpaCy\n",
        "\"\"\"\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(f'{article}')\n",
        "[t.text for t in doc]\n",
        "\n",
        "\"\"\"\n",
        "Gensim\n",
        "\"\"\"\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "simple_preprocess(f'{article}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e575f14e",
      "metadata": {
        "id": "e575f14e"
      },
      "source": [
        "\n",
        "## 7. Задача для самостоятельного разбора  \n",
        "\n",
        "Прочитать статью:  \n",
        "**\"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"** (https://arxiv.org/abs/1604.00788)\n",
        "\n",
        "### Напишите ответы на вопросы:\n",
        "1. Что значит Out-of-Vocabulary?  \n",
        "2. Как эту проблемы решили авторы статьи \"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"?\n",
        "3. Какие еще типы токенизации мы разбирали на занятии?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ваш ответ здесь:\n",
        "\n",
        "1. Это значит, что слово для модели неизвестно (оно отсутствует в словаре модели).\n",
        "2. Авторы статьи изпользовали гибридный метод: обычно модель переводит на уровне слов, обращаясь к символьному уровню, когда дело доходит до редких слов. Используемые рекуррентные нейронные сети на уровне символов вычисляют представления исходных слов и восстанавливают неизвестные целевые слова при необходимости. Символьные модели могут успешно научиться не только генерировать правильно сформированные слова для чешского языка, языка с высокой степенью словоизменения и очень сложным словарём, но и создавать правильные представления для английских исходных слов.\n",
        "3. При помощи разделения по пробелам (split) и регулярных выражений (re.findall)."
      ],
      "metadata": {
        "id": "VpPnbGafikMc"
      },
      "id": "VpPnbGafikMc"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}