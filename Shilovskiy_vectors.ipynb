{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzrMnRDOEKho"
      },
      "source": [
        "### Домашка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OI0xJWFEMbw"
      },
      "source": [
        "1. Скачать [датасет по ссылке](https://raw.githubusercontent.com/tyqiangz/multilingual-sentiment-datasets/refs/heads/main/data/english/test.csv) (просто запустить строку ниже)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHQpBRaRAUNd",
        "outputId": "beb628cc-900f-48a5-9817-b845499e68f5"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/tyqiangz/multilingual-sentiment-datasets/refs/heads/main/data/english/test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK5JMYLfEVlf"
      },
      "source": [
        "2. Открыть csv и сохранить текст в переменную (макс. балл == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "096biNetEU5G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"test.csv\")\n",
        "texts = df[\"text\"].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziGX47h2E8OE"
      },
      "source": [
        "3. Извлечь столбец с текстами (макс. балл == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2hiqxfiFBwy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('test.csv') \n",
        "texts = df['text'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhxbgwC-FCGr"
      },
      "source": [
        "4. Построить Bag-of-Words (макс. балл == 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfhUa-NuFFv8"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import string\n",
        "\n",
        "emoji_pattern = re.compile(\n",
        "    \"[\" \n",
        "    \"\\U0001F600-\\U0001F64F\"\n",
        "    \"\\U0001F300-\\U0001F5FF\"\n",
        "    \"\\U0001F680-\\U0001F6FF\"\n",
        "    \"\\U0001F700-\\U0001F77F\"\n",
        "    \"\\U0001F780-\\U0001F7FF\"\n",
        "    \"\\U0001F800-\\U0001F8FF\"\n",
        "    \"\\U0001F900-\\U0001F9FF\"\n",
        "    \"\\U0001FA00-\\U0001FA6F\"\n",
        "    \"\\U0001FA70-\\U0001FAFF\"\n",
        "    \"]+\"\n",
        ")\n",
        "\n",
        "df = pd.read_csv(\"test.csv\")\n",
        "texts = df[\"text\"].astype(str)\n",
        "\n",
        "tokenized_docs = [emoji_pattern.sub('', doc).lower().split() for doc in texts]\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "def bow_to_dense(bow_corpus, dictionary):\n",
        "    dense_vectors = []\n",
        "    for doc in bow_corpus:\n",
        "        dense_vec = [0] * len(dictionary)\n",
        "        for idx, freq in doc:\n",
        "            dense_vec[idx] = freq\n",
        "        dense_vectors.append(dense_vec)\n",
        "    return dense_vectors\n",
        "\n",
        "dense_vectors = bow_to_dense(bow_corpus, dictionary)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Визуализация\n",
        "matrix = np.array(dense_vectors)\n",
        "words = list(dictionary.token2id.keys())\n",
        "doc_names = [f\"Doc {i+1}\" for i in range(len(texts))]\n",
        "\n",
        "print(f\"Matrix shape: {matrix.shape}\")\n",
        "\n",
        "plt.figure(figsize=(25, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(matrix,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            xticklabels=words,\n",
        "            yticklabels=doc_names,\n",
        "            cmap='Blues',\n",
        "            cbar_kws={'label': 'Word Frequency'})\n",
        "plt.title('Bag-of-Words Matrix Heatmap')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Documents')\n",
        "plt.xlabel('Words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBFR3AL4FI33"
      },
      "source": [
        "5. Построить TF-IDF (макс. балл == 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75sjUGaKFLHS"
      },
      "outputs": [],
      "source": [
        "tfidf_model = TfidfModel(bow_corpus)\n",
        "tfidf_corpus = tfidf_model[bow_corpus]\n",
        "\n",
        "def tfidf_to_dense(tfidf_corpus, dictionary):\n",
        "    dense_vectors = []\n",
        "    for doc in tfidf_corpus:\n",
        "        dense_vec = [0.0] * len(dictionary)\n",
        "        for idx, score in doc:\n",
        "            dense_vec[idx] = score\n",
        "        dense_vectors.append(dense_vec)\n",
        "    return dense_vectors\n",
        "\n",
        "tfidf_dense = tfidf_to_dense(tfidf_corpus, dictionary)\n",
        "tfidf_matrix = np.array(tfidf_dense)\n",
        "words = list(dictionary.token2id.keys())\n",
        "doc_names = [f\"Док {i+1}\" for i in range(len(tfidf_corpus))]\n",
        "\n",
        "plt.figure(figsize=(40, 20))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.heatmap(tfidf_matrix,\n",
        "            annot=True,\n",
        "            fmt='.3f',\n",
        "            xticklabels=words,\n",
        "            yticklabels=doc_names,\n",
        "            cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'TF-IDF Вес'})\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "\n",
        "tf_dense = bow_to_dense(bow_corpus, dictionary)\n",
        "tf_matrix = np.array(tf_dense)\n",
        "\n",
        "doc_idx = 0\n",
        "x_pos = np.arange(len(words))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x_pos - width/2, tf_matrix[doc_idx], width, label='TF', alpha=0.7, color='blue')\n",
        "plt.bar(x_pos + width/2, tfidf_matrix[doc_idx], width, label='TF-IDF', alpha=0.7, color='red')\n",
        "plt.xticks(x_pos, words, rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Задача 2.2: Проанализируйте веса TF-IDF (макс. балл == 3)\n",
        "\n",
        "df = pd.read_csv(\"test.csv\")\n",
        "texts = df[\"text\"].astype(str)\n",
        "\n",
        "tokenized_docs = [emoji_pattern.sub('', doc).lower().split() for doc in texts]\n",
        "vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "\n",
        "tf = pd.DataFrame([\n",
        "    [doc.count(word) for word in vocab] \n",
        "    for doc in texts\n",
        "], columns=vocab)\n",
        "\n",
        "df_values = (tf > 0).sum(axis=0)\n",
        "\n",
        "N = len(tf)\n",
        "idf = np.log((N) / (df_values + 1)) + 1  \n",
        "idf = pd.Series(idf, index=vocab)\n",
        "\n",
        "tfidf = tf * idf\n",
        "\n",
        "result = pd.concat([\n",
        "    tf.T.add_prefix(\"TF_doc_\"),\n",
        "    df_values.rename(\"DF\"),\n",
        "    idf.rename(\"IDF\"),\n",
        "    tfidf.T.add_prefix(\"TFIDF_doc_\")\n",
        "], axis=1)\n",
        "\n",
        "output_path = \"result.csv\"\n",
        "result.to_csv(output_path, encoding=\"utf-8-sig\")\n",
        "print(f\"Файл сохранён: {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YgMvYrjW7l7l",
        "yFO8VSjY7rNX",
        "qTOv314oAIUT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
